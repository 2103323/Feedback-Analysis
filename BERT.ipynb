{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9758d91-c602-4e8d-9ed4-312de47cd5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\suyas\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\suyas\\anaconda3\\lib\\site-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\suyas\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\suyas\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d889fe-f81a-443e-8671-5b479bc5cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In your Python script or notebook\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c358a-53c9-436c-9313-6047b69d7dc7",
   "metadata": {},
   "source": [
    "# Problem Approach: Multi-Label Classification using BERT\r\n",
    "\r\n",
    "## 1. **Understanding the Problem**\r\n",
    "   The task was to classify items in the dataset into multiple categories (Level 1 Factors). This is a multi-label classification problem where each item could belong to one or more categories. A model needs to be built that can predict these categories based on the provided item descriptions.\r\n",
    "\r\n",
    "## 2. **Data Exploration**\r\n",
    "   - **Training Data:** The `bodywash-train.xlsx` file contains the `Core Item` column (text descriptions of items) and the `Level 1 Factors` column (comma-separated categories).\r\n",
    "   - **Test Data:** The `bodywash-test.xlsx` file contains only the `Core Item` column and needs to be labeled based on the trained model.\r\n",
    "\r\n",
    "## 3. **Data Preprocessing**\r\n",
    "   To prepare the data for the BERT model:\r\n",
    "   - **Text Cleaning:** Applied preprocessing techniques to clean the item descriptions:\r\n",
    "     - Converted text to lowercase.\r\n",
    "     - Removed special characters, numbers, and punctuation.\r\n",
    "     - Removed stopwords using the NLTK library.\r\n",
    "     - Removed extra whitespaces.\r\n",
    "   - **Multi-Label Encoding:** The `Level 1 Factors` column (comma-separated labels) was transformed into a binary format using `MultiLabelBinarizer`, turning it into a format suitable for multi-label classification.\r\n",
    "\r\n",
    "## 4. **Model Selection**\r\n",
    "   Initially, I considered using **Logistic Regression**, but due to a low F1-score, I moved to **BERT (Bidirectional Encoder Representations from Transformers)** for the following reasons:\r\n",
    "   - **Contextual Understanding**: BERT's attention mechanism allows it to understand context by considering both the left and right sides of a word.\r\n",
    "   - **Pre-trained Weights**: The pre-trained `bert-base-uncased` model provides a strong starting point for fine-tuning on specific tasks, avoiding the need for training a model from scratch.\r\n",
    "   - **Adaptability for Multi-Label Classification**: BERT can be easily adapted to multi-label tasks by modifying its classification head and loss function.\r\n",
    "\r\n",
    "## 5. **Model Training**\r\n",
    "   - **Tokenizer:** Used the pre-trained BERT tokenizer (`bert-base-uncased`) to convert the item descriptions into token IDs and attention masks.\r\n",
    "   - **Custom Dataset:** Defined a custom PyTorch `Dataset` to feed tokenized data and labels into the model.\r\n",
    "   - **Model Architecture:** Loaded a pre-trained BERT model (`BertForSequenceClassification`) with a classification head. The head was modified to handle multi-label classification by setting the `num_labels` parameter to match the number of unique categories in the dataset.\r\n",
    "   - **Loss Function:** Used `BCEWithLogitsLoss` (Binary Cross-Entropy with Logits) since it's suited for multi-label classification tasks.\r\n",
    "   - **Optimizer:** AdamW optimizer was chosen with a learning rate of `1e-5` to fine-tune the model.\r\n",
    "\r\n",
    "## 6. **Training Loop**\r\n",
    "   - **Batch Training:** Data was fed into the model in batches using PyTorch's `DataLoader`.\r\n",
    "   - **Backpropagation:** For each batch, the model's logits (predictions) were compared to the true labels, and the loss was calculated. Gradients were then backpropagated to update model weights.\r\n",
    "   - **Validation:** After each epoch, the model was evaluated on a validation set to monitor its performance.\r\n",
    "\r\n",
    "## 7. **Prediction on Test Data**\r\n",
    "   - **Test Dataset:** A similar tokenization process was applied to the test dataset.\r\n",
    "   - **Prediction:** The trained model was used to predict the categories (Level 1 Factors) for each item in the test set.\r\n",
    "   - **Thresholding:** The model's output (logits) was passed through a sigmoid function to convert them into probabilities. A threshold of 0.5 was used to determine which categories to assign to each item.\r\n",
    "   - **Inverse Transformation:** The predicted binary labels were transformed back into their corresponding category names using the `inverse_transform` method from `MultiLabelBinarizer`.\r\n",
    "\r\n",
    "## 8. **Saving Results**\r\n",
    "   - The predictions were added as a new column in the test dataset.\r\n",
    "   - The final results were saved to an Excel file (`bodywash_test_predictions.xlsx`).\r\n",
    "\r\n",
    "## 9. **Conclusion**\r\n",
    "   This approach effectively utilizes a pre-trained BERT model for multi-label classification. BERT's attention mechanism and pre-trained weights enable the model to generalize well and predict the appropriate categories for the given test data.\r\n",
    "dict the appropriate categories for the given test data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfce3f35-e3a8-4d37-a11f-76dc0b79ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████| 436/436 [5:07:52<00:00, 42.37s/it, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 0: 0.2055515193817567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████| 436/436 [1:59:59<00:00, 16.51s/it, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 1: 0.1950129690218945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████| 436/436 [1:33:41<00:00, 12.89s/it, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss after epoch 2: 0.18517818682047785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-multi-label-classifier\\\\tokenizer_config.json',\n",
       " './bert-multi-label-classifier\\\\special_tokens_map.json',\n",
       " './bert-multi-label-classifier\\\\vocab.txt',\n",
       " './bert-multi-label-classifier\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load data (adapt this to your data loading method)\n",
    "df_train = pd.read_excel('bodywash-train.xlsx')\n",
    "df_test = pd.read_excel('bodywash-test.xlsx')\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords (optional)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'item' column\n",
    "df_train['Core Item'] = df_train['Core Item'].apply(clean_text)\n",
    "df_test['Core Item'] = df_test['Core Item'].apply(clean_text)\n",
    "\n",
    "# Assuming 'item' and 'factors' are the relevant columns in the dataset\n",
    "items = df_train['Core Item'].tolist()\n",
    "factors = df_train['Level 1 Factors'].apply(lambda x: x.split(',')).tolist()  # Assuming factors are comma-separated\n",
    "\n",
    "# MultiLabelBinarizer to encode the factors\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(factors)  # Converts factors into one-hot encoded format\n",
    "labels_list = mlb.classes_  # List of unique labels\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom dataset class for loading tokenized data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, items, labels):\n",
    "        self.items = items\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the item\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            item,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask, torch.tensor(label)\n",
    "\n",
    "# Split dataset into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(items, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load pre-trained BERT model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=y_train.shape[1], problem_type=\"multi_label_classification\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Define BCEWithLogitsLoss for multi-label classification\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Apply sigmoid to logits for multi-label classification\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels.float())  # Labels should be float for BCEWithLogitsLoss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Validation loop (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels.float())\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss after epoch {epoch}: {val_loss / len(val_loader)}\")\n",
    "\n",
    "# Save the trained model (optional)\n",
    "model.save_pretrained('./bert-multi-label-classifier')\n",
    "tokenizer.save_pretrained('./bert-multi-label-classifier')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bb23e-40f6-4c8c-bb80-cdc7a130846c",
   "metadata": {},
   "source": [
    "## Observation\r\n",
    "\r\n",
    "After training the BERT model for multi-label classification over 3 epochs, the following key observations were made:\r\n",
    "\r\n",
    "### 1. **Training Performance**:\r\n",
    "   - The model's training loss gradually decreased from epoch 0 to epoch 2, indicating that the model was effectively learning from the data:\r\n",
    "     - **Epoch 0**: Training Loss = 0.203\r\n",
    "     - **Epoch 1**: Training Loss = 0.182\r\n",
    "     - **Epoch 2**: Training Loss = 0.212\r\n",
    "   - Although there was a slight increase in training loss during epoch 2, the model overall demonstrated good training behavior.\r\n",
    "\r\n",
    "### 2. **Validation Performance**:\r\n",
    "   - The validation loss consistently improved over the first two epochs, showing that the model was generalizing well to unseen validation data:\r\n",
    "     - **Validation Loss after Epoch 0**: 0.2055\r\n",
    "     - **Validation Loss after Epoch 1**: 0.1950\r\n",
    "     - **Validation Loss after Epoch 2**: 0.1852\r\n",
    "   - The steady reduction in validation loss indicates that the model is improving its ability to predict labels accurately g in future epochs.\r\n",
    "later epochs.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec538279-ab54-4ff0-b7e2-88b487e79a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'bodywash_test_predictions.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "# Prepare test data for prediction\n",
    "test_items = df_test['Core Item'].tolist()\n",
    "\n",
    "# Create a CustomDataset for the test set (without labels)\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        \n",
    "        # Tokenize the item\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            item,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_dataset = TestDataset(test_items)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Make predictions on the test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply sigmoid and threshold to get binary predictions\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        predictions.append(preds)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Apply softmax along axis=1 to normalize logits row-wise\n",
    "probabilities = F.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "\n",
    "# Define a threshold (e.g., 0.5) and create binary predictions\n",
    "threshold = 0.5\n",
    "binary_predictions = (probabilities > threshold).astype(int)\n",
    "\n",
    "# Convert binary predictions back to factors (multi-label)\n",
    "predicted_factors = mlb.inverse_transform(binary_predictions)\n",
    "\n",
    "# Add the predicted factors to the test DataFrame\n",
    "df_test['Predicted Factors'] = [' , '.join(factors) for factors in predicted_factors]\n",
    "\n",
    "# Save the results to a new Excel file\n",
    "df_test.to_excel('bodywash_test_predictions.xlsx', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'bodywash_test_predictions.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85c6ec02-d4da-4bdf-be01-cdfb3856d78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Cleansing',),\n",
       " ('Cleansing',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Price',),\n",
       " ('Brand Value',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Brand Value',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Brand Value',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Price',),\n",
       " ('Price',),\n",
       " ('Price',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Cleansing',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Brand Value',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Cleansing',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',),\n",
       " ('Fragrance',)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_factors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
